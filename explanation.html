<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

</head>
<div class="jumbotron">
  <div class="container">
<h2>This is the Hill We Want to Climb On: The Painting Project</h2>
<p class="lead">Our project develops a fascimile of a given image, all by itself. But how?</p>
</div>
</div>
<div class="container-fluid">
  <div class="row">
    <div class="col-sm-1"></div>
    <div class="col-lg-1">
<body>
  <b>The below explanation requires zero coding knowledge!!!</b>
  <p>
    As you can see in the videos, our software magically replicates a chosen image over time.
    This is done through a complex series of trial-and-error.
    Each version the computer displays gets slightly more similar to the original image.
    Pretty cool, right?
  </p>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/HNFxnlvDnFA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/wIB9WVDRg3I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/BKYhjvE6-Is" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
  <p>
    <i>It should be duly noted that this actually took seven hours to run! We felt you might not want to watch a seven hour video.</i>
  </p>
  <h3>Polygon with the wind</h3>
  <p>Each computer-generated image you see cycling through is made up of a number of polygons.
  At the start of each experiment, we (the coders) pick the number of polygons in the image,
  how many vertices each polygon has, and whether the background is white or black. The computer
  randomly creates an image made up of our chosen number of polygons. Each polygon has a randomly chosen
  color, transparency, and location (the coordinates of each vertex.)
  The term 'image' is used a bit misleadingly here. Each image the computer creates is actually
  just a visual representation of the list of polygons the computer has created, called an array.
  Think of it like a spreadsheet, with each row representing a different polygon, with a column each for color,
  transparency, and the pixel coordinates of each of its vertices. The computer reads the spreadsheet extremely quickly
  and renders it for each iteration of the painting process. So we'll use "collection of polygons", "iteration",
  and "image" interchangeably in this explainer.</p>
  <h3>Fitness (with a brief interlude into the fact that Computers Lack Actual Eyes)</h3>
  <p>In order to get closer and closer to a copy of the target image, the computer must have a way
  of seeing how accurate the current combination of polygons is. A human could simply glance at the original
  and see what was inaccurate or in need of further detail, but software does not (really) have the capability
  for qualitative analysis like this. We use the fitness function to figure out how to go from one iteration to the next. To get better, we must have a way of
  deciding what 'better' really is.
  After consulting the literature, we chose a function that finds the sums of the difference between each pixel in the two images.
  The lower the sum, the more 'fit' the image. Extrapolating from the Pythagorean theorem, we sum the differences between green, red, blue, and transparency
  numeric values for, say, pixel 1 in the original image and pixel 1 in the computer image, and add the square root of all that to the final tally.
  This is one of the moments we are very, very grateful for just how fast computers can do arithmetic.
  The emerging field of computer vision, computers analyzing an image in its
  entirety instead of pixel-by-pixel, does have potential solutions to this, but they are well beyond the scope of
  this class and much harder to execute. A computer can play a Highlights magazine style spot-the-difference game holistically,
  instead of counting the number of differing pixels saying "there's a blob in image A and not in image B", but it can't quite yet
  correct the discrepencies through the building-polygons method described above. Thus, we at RCS AI delved into the
  world of fitness functions: mathematical formulas that spit out the 'fitness' of an attempted fascimile like this one.
  </p>
  <h3>teenage mutant ninja functions</h3>
  <p>So how does the array change? There's no set way for an artificial intelligence algorithm to do this.
  Each method has to be specific to the scenario, whether the computer is painting, writing a Saturday Night Live sketch,
  or developing a cancer drug chemical formula. We wrote different options: some change the location of all the shapes
  by a set amount, some randomly change the color and transparency of all, and some alter the characteristics of a subset
  of the shapes. We use a combination of these techniques during different phases of the painting process (which the computer detects; see below)
  in order to produce the most accurate result possible. If the newly created image is better than the previous, it becomes the 'current' image.
  </p>

  <h3>The hills are alive....</h3>
  <h3>bonus round: deathmatch!!!!</h3>
  <p>
    The infrastructure of generating polygons, visualizing them, running the fitness and mutation functions, and moving on to better
    versions took a few weeks to build, so once it finally worked we spent a few days entranced by the flashing shapes that eventually
    looked like the images we eagerly inputted. We putzed around with different types of mutation functions, eventually settling on the
    multiple-functions-for-different-needs strategy mentioned above. However, as our time on the project came to a close we began looking
    into traditional genetic-algorithm methods of different versions competing against each other, in a sort of natural-selection/death-match
    type thing.
  </p>
  <h2>This is cool and all but what's the point?</h2>
  <p>Hill climbing, fitness functions, and mutation functions are the essential building blocks of the most influential breakthrough in artificial intelligence,
  genetic algorithms. Classical genetic algorithmic solutions to this painting problem have multiple hill-climber iterators running at once, then combining
  the best elements of all the different contests to create the contestants for the next round. The program stops after the differences between 'best versions' of
  different rounds becomes very, very small. This system in turn powers the 'neural networks' your mother warned you about. Just kidding.
  If you're able to read this website, you likely weren't born before 2012, when neural networks moved from 'cool idea' to 'wait this works'
territory. Spookishly, we don't know why they work so well, at tasks like image identification, playing chess and video games, and solving large problems
in medicine, government, and across the sciences. This project was at the end of a year's worth of graduate-level AI algorithm study, and we're very proud
of cooking up the ingredients to neural nets ourselves, bringing our skills to right at the forefront of computer science research.  </p>
  <h2>so will AI kill us all?</h2>
  <p>If Haley Joel Osment can survive a Jude Law sci-fi Pinnochio movie, we can survive the future of the internet.
    In all seriousness, the algorithm works better on some images than others. As you'll notice in the above videos, the algorithm is able to
    be more accurate when given target images like the Google Chrome logo, with blocks of color and not too many details. The high-resolution photograph of
    our headmaster Dominic Andrew Affleck Randolph struggled to make out details like his eyes and nose. The Mona Lisa was somewhere in the middle- notice
    background's colors and shapes are nearly indistinguishable from a distance, while she lacks a nose or mouth.
    </thead>
  </p>
  <h2>References</h2>
  <p>
    We were inspired by the examples made by <a href="http://alteredqualia.com/visualization/evolve/">Altered Qualia</a>
    and <a href="https://chriscummins.cc/s/genetics/#">Chris Cummins.</a> Special thanks to <a href="https://rogerjohansson.blog/2008/12/09/genetic-programming-mona-lisa-faq/">
    Roger Johansson</a> for deciding on the most effective fitness function for us.
  </p>
  <p>
    <i>
      The Riverdale Country School 2017-2018 Artificial Intelligence class is Will Holzman '19, Nolan Flynn '19, and Katie Orenstein '18,
      with faculty advisor Alex Kuntz. Explainer written by Katie.  </i>
      <a href="http://riverdale.edu">Check out our school!</a>
    </p>
  </div>

</div>
</body>

</html>
